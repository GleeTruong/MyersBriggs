{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stopped-operator",
   "metadata": {
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1618167125588,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "brown-bronze"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import gensim\n",
    "import random\n",
    "import ast\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-albert",
   "metadata": {
    "id": "graphic-addiction"
   },
   "source": [
    "### Vader Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "phantom-terrain",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "error",
     "timestamp": 1618167232436,
     "user": {
      "displayName": "Soon Chye Lim",
      "photoUrl": "",
      "userId": "02286047641686637328"
     },
     "user_tz": 420
    },
    "id": "stylish-vietnamese",
    "outputId": "a2fab05e-f887-4db1-a4cf-ebc7ec26ed35"
   },
   "outputs": [],
   "source": [
    "df1=pd.read_csv('Clean_v1.csv')\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "def vaderScore(comments):\n",
    "    comments = re.sub('\\w+:\\/\\/\\S+','',comments)\n",
    "    comments = re.sub('\\.\\.\\.','', comments)\n",
    "    comments = re.sub('[0-9]+','', comments)\n",
    "    comments = re.sub('\\|\\|\\|', '', comments)\n",
    "    comments = sia.polarity_scores(comments)\n",
    "    return comments\n",
    "\n",
    "df1['vaderScore']=df1['posts'].apply(lambda x: vaderScore(x))\n",
    "vaderDF =df1[[\"Unnamed: 0\",'type','vaderScore']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-implementation",
   "metadata": {
    "id": "basic-hamilton"
   },
   "source": [
    "### Emoji Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "addressed-intake",
   "metadata": {
    "id": "about-amber"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>Clean</th>\n",
       "      <th>New</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>INFJ</td>\n",
       "      <td>' and intj moments    sportscenter not top ten...</td>\n",
       "      <td>['intj', 'moment', 'sportscenter', 'top', 'ten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "      <td>['m', 'find', 'lack', 'post', 'alarmingsex', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____    course, to which I say I k...</td>\n",
       "      <td>['good', 'one', 'course', 'say', 'know', 's', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>['dear', 'intp', 'enjoyed', 'conversation', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.That's another silly misconcepti...</td>\n",
       "      <td>['you', 're', 'firedthat', 's', 'another', 'si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>8670</td>\n",
       "      <td>ISFP</td>\n",
       "      <td>' just because I always think of cats as Fi do...</td>\n",
       "      <td>['always', 'think', 'cat', 'fi', 'doms', 'reas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8671</th>\n",
       "      <td>8671</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>'Soif this thread already exists someplace els...</td>\n",
       "      <td>['soif', 'thread', 'already', 'exist', 'somepl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>8672</td>\n",
       "      <td>INTP</td>\n",
       "      <td>'So many questions when i do these things.  I ...</td>\n",
       "      <td>['so', 'many', 'question', 'thing', 'would', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8673</th>\n",
       "      <td>8673</td>\n",
       "      <td>INFP</td>\n",
       "      <td>'I am very conflicted right now when it comes ...</td>\n",
       "      <td>['conflict', 'right', 'come', 'want', 'child',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8674</th>\n",
       "      <td>8674</td>\n",
       "      <td>INFP</td>\n",
       "      <td>'It has been too long since I have been on per...</td>\n",
       "      <td>['it', 'long', 'since', 'personalitycafe', 'al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8675 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  type                                              Clean  \\\n",
       "0              0  INFJ  ' and intj moments    sportscenter not top ten...   \n",
       "1              1  ENTP  'I'm finding the lack of me in these posts ver...   \n",
       "2              2  INTP  'Good one  _____    course, to which I say I k...   \n",
       "3              3  INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "4              4  ENTJ  'You're fired.That's another silly misconcepti...   \n",
       "...          ...   ...                                                ...   \n",
       "8670        8670  ISFP  ' just because I always think of cats as Fi do...   \n",
       "8671        8671  ENFP  'Soif this thread already exists someplace els...   \n",
       "8672        8672  INTP  'So many questions when i do these things.  I ...   \n",
       "8673        8673  INFP  'I am very conflicted right now when it comes ...   \n",
       "8674        8674  INFP  'It has been too long since I have been on per...   \n",
       "\n",
       "                                                    New  \n",
       "0     ['intj', 'moment', 'sportscenter', 'top', 'ten...  \n",
       "1     ['m', 'find', 'lack', 'post', 'alarmingsex', '...  \n",
       "2     ['good', 'one', 'course', 'say', 'know', 's', ...  \n",
       "3     ['dear', 'intp', 'enjoyed', 'conversation', 'd...  \n",
       "4     ['you', 're', 'firedthat', 's', 'another', 'si...  \n",
       "...                                                 ...  \n",
       "8670  ['always', 'think', 'cat', 'fi', 'doms', 'reas...  \n",
       "8671  ['soif', 'thread', 'already', 'exist', 'somepl...  \n",
       "8672  ['so', 'many', 'question', 'thing', 'would', '...  \n",
       "8673  ['conflict', 'right', 'come', 'want', 'child',...  \n",
       "8674  ['it', 'long', 'since', 'personalitycafe', 'al...  \n",
       "\n",
       "[8675 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_v2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "human-resort",
   "metadata": {
    "id": "concrete-pointer"
   },
   "outputs": [],
   "source": [
    "words = df['New']\n",
    "posts = []\n",
    "for i in range(len(words)):\n",
    "    pre = words[i]\n",
    "    lst = eval(pre)\n",
    "    post = ''\n",
    "    for word in lst:\n",
    "        post += word + ' '\n",
    "    posts.append(post)\n",
    "\n",
    "emotes_list = []\n",
    "for i in posts:\n",
    "    emotes = re.findall('\\<.{2,25}?\\>', i)\n",
    "    for i in emotes:\n",
    "        if i.count('<')+i.count('>')>2: #more filters\n",
    "            emotes.remove(i)\n",
    "    for j in emotes:\n",
    "        emotes_list.append(j)\n",
    "fd = nltk.FreqDist(emotes_list)\n",
    "common_emojis = [x for x,y in fd.most_common(40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "accepted-newspaper",
   "metadata": {
    "id": "split-cameroon"
   },
   "outputs": [],
   "source": [
    "def emojiClean(comments):\n",
    "    comments = re.sub(\"\\[\", '',comments)\n",
    "    comments = re.sub(\"\\]\", '',comments)\n",
    "    comments = re.sub(\"\\'\", '',comments)\n",
    "    comments = re.sub(\"\\,\", '',comments)\n",
    "    return comments\n",
    "\n",
    "def feature_extractor(doc):\n",
    "    features = {}\n",
    "    for e in common_emojis:\n",
    "        features[e] = (e in doc)\n",
    "    return features\n",
    "\n",
    "df['Emoji']=df['New'].apply(lambda x: emojiClean(x))\n",
    "df['EmojiFeature'] = df['Emoji'].apply(lambda x: feature_extractor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-robertson",
   "metadata": {
    "id": "correct-letters"
   },
   "source": [
    "### Common ADJ Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "solved-junior",
   "metadata": {
    "id": "dangerous-planning"
   },
   "outputs": [],
   "source": [
    "stopwordlist = gensim.parsing.preprocessing.STOPWORDS\n",
    "def cleaner(post):\n",
    "    # remove words in < > or special symbols residuals\n",
    "    special_symbols_remove = re.sub('\\<[A-Za-z]+.[\\s\\w]*\\>|\\(|\\)|\\*|[A-Z]\\/[A-Z]|\\_+', '', post)\n",
    "    wordlist = nltk.word_tokenize(special_symbols_remove)\n",
    "    words_nonstop = [word.lower() for word in wordlist if word.lower() not in stopwordlist or word not in string.punctuation]\n",
    "    words = [word for word in words_nonstop if len(word) > 3] # words that length than 3\n",
    "    return words\n",
    "\n",
    "def feature_common_adjs(words):\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    adjs = [x for x, y in tagged if y.startswith('JJ') and x.isalpha()] # prevent uncleaned words\n",
    "    common_adjs = [x for x, y in nltk.FreqDist(adjs).most_common(200)]\n",
    "    return common_adjs\n",
    "\n",
    "def check_adjs(post):\n",
    "    adj_in_type = [w for w in post if w in adjs]\n",
    "    return {'AdjInTop200 ': len(adj_in_type)}\n",
    "\n",
    "df['Cleaner'] = df['Clean'].apply(lambda post: cleaner(post))\n",
    "corpus = [word for post in df['Cleaner'] for word in post] # total 5994734\n",
    "adjs = feature_common_adjs(corpus)\n",
    "df['ADJs'] = df['Cleaner'].apply(lambda post: check_adjs(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-henry",
   "metadata": {
    "id": "numerical-telescope"
   },
   "source": [
    "### Most Common Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cleared-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word = []\n",
    "for i in range(len(df['Clean'])):\n",
    "    word = cleaner(df['Clean'][i])\n",
    "    for j in word:\n",
    "        all_word.append(j)\n",
    "       \n",
    "fd = nltk.FreqDist(all_word)      \n",
    "most_common = fd.most_common(200)\n",
    "most = [z[0] for z in most_common]\n",
    "\n",
    "# Set the feature\n",
    "def most_feature(post):\n",
    "    count = 0\n",
    "    words = set(post)\n",
    "    feature = {}\n",
    "    for w in words:\n",
    "        if w in most:\n",
    "            count = count + 1\n",
    "            feature['CommonWordCount'] = count\n",
    "        else:\n",
    "            feature['CommonWordCount'] = count\n",
    "    return feature\n",
    "\n",
    "df['MostCommonWord'] = df['Cleaner'].apply(lambda post: most_feature(post))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-joshua",
   "metadata": {},
   "source": [
    "### Avg Length in the Sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "genuine-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sent):\n",
    "    str1 = sent.strip() # remove the space at start and end of string\n",
    "    index = 0\n",
    "    count = 0\n",
    "    \n",
    "    while index < len(str1):\n",
    "        while str1[index] != \" \": \n",
    "            index += 1   \n",
    "            if index == len(str1): \n",
    "                break\n",
    "        count += 1 # count number of word\n",
    "        if index == len(str1): # check if the character is last one\n",
    "            break\n",
    "        while str1[index] == \" \":  # check the space between the word\n",
    "            index += 1\n",
    "    \n",
    "    # the number of sentence of post\n",
    "    num_sent = len(re.split(r'[.!?]+',sent))\n",
    "    # calculate the average word per sentence\n",
    "    avg_word_length = count/num_sent\n",
    "\n",
    "    return round(avg_word_length,1)\n",
    "\n",
    "def avg_feature(num):\n",
    "    feature = {} \n",
    "    feature['AvgSentLength'] = num\n",
    "    return feature\n",
    "\n",
    "df['AvgSentLength'] = df['Clean'].apply(lambda post: avg_word(post))\n",
    "df['AvgSentLength'] = df['AvgSentLength'].apply(lambda post: avg_feature(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-samoa",
   "metadata": {},
   "source": [
    "### TFIDF Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-lecture",
   "metadata": {
    "id": "written-cuisine",
    "outputId": "aba12b51-8bb5-466b-9b4e-4f36eecca732"
   },
   "outputs": [],
   "source": [
    "### Run if you have 18 hrs to spare.... if not just import from pickle file: PickleFeatures\n",
    "vectorizer=TfidfVectorizer()\n",
    "tfidf=vectorizer.fit_transform(posts)\n",
    "num_word=len(vectorizer.get_feature_names())\n",
    "\n",
    "def feature_function(post_index):\n",
    "    sum_tfidf=sum([tfidf[post_index,word_index] for word_index in range(0,num_word)])\n",
    "    length=len(nltk.word_tokenize(posts[post_index]))\n",
    "    return {'avg_tfidf': round(sum_tfidf,3)}\n",
    "\n",
    "lst =[]\n",
    "for i in range(8675):\n",
    "    lst.append(feature_function(i))\n",
    "    \n",
    "df[\"Avg TFIDF\"]=lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-resident",
   "metadata": {},
   "source": [
    "### AVG Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "turned-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgChar(doc):\n",
    "    lst = eval(doc)\n",
    "    lst = [x for x in lst if x.isalpha() and len(x)>2]\n",
    "    word_no = len(lst)\n",
    "    count = 0\n",
    "    for i in lst:\n",
    "        count += len(i)\n",
    "    try:\n",
    "        avgChar = count/word_no\n",
    "    except ZeroDivisionError:\n",
    "        avgChar = 0\n",
    "    return {'AvgChar': round(avgChar,2)}\n",
    "df['AvgChar'] = df['New'].apply(lambda x: avgChar(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-principle",
   "metadata": {},
   "source": [
    "### Joining the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "alone-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the df and vaderDF because they are both using different clean files.\n",
    "dfMerge = pd.merge(df, vaderDF, on=['Unnamed: 0','type'], how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "upset-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this instead of running the AVG TFIDF FUNCTION\n",
    "pickleDF = pd.read_pickle('PickleFeatures')\n",
    "pickleDF =pickleDF[['Unnamed: 0','type','Avg TFIDF']]\n",
    "dfMerge = pd.merge(dfMerge, pickleDF, on=['Unnamed: 0','type'], how ='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "horizontal-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subeset dfMerge and grabbed only the features ouput\n",
    "featuresDF=dfMerge[['type','EmojiFeature','ADJs','AvgSentLength','MostCommonWord','AvgChar', 'Avg TFIDF','vaderScore']]\n",
    "featuresDF.to_pickle(\"PickleFeaturesV2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-broadway",
   "metadata": {},
   "source": [
    "### Combing Features to Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('PickleFeaturesV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a new column to df\n",
    "df['CombineDict']= ''\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df['CombineDict'][i]= ({**df['EmojiFeature'][i], **df['ADJs'][i],**df['AvgSentLength'][i],**df['MostCommonWord'][i],\n",
    "                            **df['AvgChar'][i], **df[\"Avg TFIDF\"][i], **df['vaderScore'][i]}, df['type'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"PickleFeatureSet\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CombinedFeatures.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
